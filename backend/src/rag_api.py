# backend/src/rag_api.py
import os
from pathlib import Path
from dotenv import load_dotenv
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ===========================
# Load bi·∫øn m√¥i tr∆∞·ªùng
# ===========================
BASE_DIR = Path(__file__).resolve().parents[1]
load_dotenv(BASE_DIR / ".env")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("‚ö†Ô∏è OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong file .env")

# ===========================
# Config model
# ===========================
MODEL_NAME = "gpt-3.5-turbo"  # ho·∫∑c gpt-4
EMBEDDING_MODEL = "text-embedding-3-small"

# ===========================
# FAISS index path
# ===========================
FAISS_INDEX_PATH = BASE_DIR / "models" / "diabetes_faiss_index"

# ===========================
# Load FAISS index
# ===========================
print(f"üìÇ ƒêang load FAISS index t·ª´: {FAISS_INDEX_PATH}")
vectorstore = None
qa_chain = None
try:
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)
    vectorstore = FAISS.load_local(
        str(FAISS_INDEX_PATH),
        embeddings,
        allow_dangerous_deserialization=True
    )
    print("‚úÖ FAISS index load th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªói khi load FAISS index: {e}")

# ===========================
# Prompt template
# ===========================
prompt_template = """
B·∫°n l√† m·ªôt chuy√™n gia y t·∫ø v·ªÅ b·ªánh ti·ªÉu ƒë∆∞·ªùng.
D·ª±a v√†o th√¥ng tin trong ph·∫ßn "Ng·ªØ c·∫£nh" b√™n d∆∞·ªõi, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
- Tr·∫£ l·ªùi chi ti·∫øt, r√µ r√†ng, ƒë·∫ßy ƒë·ªß.
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn ho·∫∑c th√¥ng tin kh√¥ng c√≥, h√£y n√≥i r√µ.

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi:
{question}

Tr·∫£ l·ªùi:
"""
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template
)

# ===========================
# Init LLM + RetrievalQA
# ===========================
if vectorstore:
    llm = ChatOpenAI(
        model=MODEL_NAME,
        temperature=0,
        api_key=OPENAI_API_KEY
    )
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

# ===========================
# API Router
# ===========================
router = APIRouter()

class QuestionRequest(BaseModel):
    question: str = Field(None, description="C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng")
    query: str = Field(None, description="Alias cho 'question', ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi frontend c≈©")

@router.post("/ask")
async def ask_question(req: QuestionRequest):
    if not qa_chain:
        raise HTTPException(status_code=500, detail="FAISS index ch∆∞a s·∫µn s√†ng.")

    # ∆Øu ti√™n l·∫•y 'question', fallback sang 'query'
    user_question = req.question or req.query
    if not user_question:
        raise HTTPException(status_code=422, detail="C·∫ßn truy·ªÅn 'question' ho·∫∑c 'query'.")

    try:
        result = qa_chain.invoke({"query": user_question})
        return {"answer": result.get("result", "")}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
