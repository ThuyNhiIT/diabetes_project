import os
from pathlib import Path
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# Load biáº¿n mÃ´i trÆ°á»ng
BASE_DIR = Path(__file__).parents[1]
load_dotenv(BASE_DIR / ".env")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh trong .env")

DATA_PATH = BASE_DIR / "data" / "diabetes_knowledge.txt"
INDEX_PATH = BASE_DIR / "models" / "diabetes_faiss_index"

print("ğŸ“„ Äang Ä‘á»c dá»¯ liá»‡u tá»«:", DATA_PATH)
with open(DATA_PATH, "r", encoding="utf-8") as f:
    raw_text = f.read()

# Chia nhá» vÄƒn báº£n
print("âœ‚ï¸ Äang chia nhá» dá»¯ liá»‡u...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.create_documents([raw_text])

# Táº¡o embeddings & FAISS index
print("âš™ï¸ Äang táº¡o embeddings vÃ  FAISS index...")
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
vectorstore = FAISS.from_documents(docs, embeddings)

# LÆ°u index
print("ğŸ’¾ Äang lÆ°u index vÃ o:", INDEX_PATH)
vectorstore.save_local(str(INDEX_PATH))
print("âœ… HoÃ n thÃ nh táº¡o FAISS index!")
